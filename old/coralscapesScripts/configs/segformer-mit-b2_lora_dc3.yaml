run_name: "segformer_mit_b2_lora_dc3_run"

data:
  batch_size: 8
  batch_size_eval: 8

augmentation:
  train:
    Resize:
      width: 1036
      height: 518
    HorizontalFlip:
      p: 0.5
    RandomCrop:
      width: 518
      height: 518
    Normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
  val:
    Resize:
      width: 1036
      height: 518
    Normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
  test:
    Resize:
      width: 1036
      height: 518
    Normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

model:
  name: "segformer-mit-b2"

training:
  epochs: 1000
  optimizer:
    type: torch.optim.AdamW
    lr: 0.0005
    weight_decay: 0.01
  scheduler:
    type: torch.optim.lr_scheduler.PolynomialLR
    power: 1
    total_iters: 1000
  preprocessor: "segformer"
  eval:
    sliding_window: True
    window: 518
    stride: 518

lora:
  r: 128
  lora_alpha: 32
  modules_to_save: ["decode_head"]


