run_name: coral_bleaching_dpt_dinov2

model:
  name: dpt-dinov2-giant
  checkpoint: null  # Will use the default backbone
  kwargs:
    num_labels: 40  # Use original 40 classes, then map to 3

lora:  # Add LoRA configuration
  r: 16  # LoRA rank
  lora_alpha: 32  # LoRA alpha scaling
  dropout: 0.1
  target_modules:
    - "query"
    - "value"
    - "key"
    - "output"
  bias: "none"

data:
  root: ../coralscapes
  batch_size: 2
  batch_size_eval: 1
  weight: true  # Use class weights for imbalanced data

training:
  epochs: 50
  preprocessor: "dpt"
  optimizer:
    type: torch.optim.AdamW
    lr: 0.00005
    weight_decay: 0.01
    lr_multiplier: 0.1
    backbone_regex: "dino" 
  scheduler:
    type: torch.optim.lr_scheduler.PolynomialLR
    power: 1
    total_iters: 50
  eval: 
    transform_target: False

augmentation:
  train:
    Resize:
      height: 224
      width: 224
    HorizontalFlip:
      p: 0.5
    VerticalFlip:
      p: 0.5
    RandomRotate90:
      p: 0.5
    RandomBrightnessContrast:
      p: 0.2
    Normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
  val:
    Resize:
      height: 224
      width: 224
    Normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
  test:
    Resize:
      height: 224
      width: 224
    Normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

logger:
  wandb_project: null
  log_epochs: 1
