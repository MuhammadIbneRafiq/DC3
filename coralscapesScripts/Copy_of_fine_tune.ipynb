{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRJVLuDR_TTl"
   },
   "source": [
    "## Note\n",
    "This notebook serves as a mini tutorial on what you need to fine tune the model on a custom dataset in the simplest way.\n",
    "\n",
    "The main things that need to be done to do so is to create a dataset class following the implementation of the Coralscapes class and to add the model checkpoint in the arguments of the training script or in the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running the finetune script on top of this"
   ],
   "metadata": {
    "id": "T8nc8abMAXPv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hyl3wBc-_5TQ",
    "outputId": "70e461c3-ad18-4d97-a0cd-940417adf24f",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:39.822630Z",
     "start_time": "2025-10-14T21:28:39.818621Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# %cd /content/drive/MyDrive/Data\\ Challenge\\ 3\\ Group\\ 3\n",
    "# !git clone https://github.com/eceo-epfl/coralscapesScripts.git"
   ],
   "metadata": {
    "id": "aU7aY_5v_UyZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4c8d9ca0-f1b6-4803-cd62-385963d96fa6",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:39.849572Z",
     "start_time": "2025-10-14T21:28:39.846210Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "# %cd coralscapesScripts",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SmSodjNLZ4YA",
    "outputId": "9c3f795a-aba5-4803-d08f-ac9f38ad3be1",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:39.864585Z",
     "start_time": "2025-10-14T21:28:39.861583Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "# !pip install albumentations transformers torch segmentation_models_pytorch torchmetrics peft matplotlib\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "_0VpRgoli8Mh",
    "outputId": "ddff1d7b-e352-4288-c051-79f095fd934d",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:39.873039Z",
     "start_time": "2025-10-14T21:28:39.871123Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDgtkKb7_TTo"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Je2z41FU_TTo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ecee23f8-d73f-4315-bbe7-86910ff9ddfb",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:39.883221Z",
     "start_time": "2025-10-14T21:28:39.878047Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# cur = os.path.join(os.getcwd(), '..')\n",
    "# print(cur)\n",
    "# proj_path = os.path.abspath(\n",
    "#     os.path.join(os.getcwd(), '..')\n",
    "# )\n",
    "# coralpath = os.path.join(proj_path, 'coralscapesScripts')\n",
    "# print(coralpath)\n",
    "#\n",
    "# sys.path.insert(0, coralpath)\n",
    "# print(\n",
    "#     sys.path\n",
    "# )# # Assume your notebook is in <project_root>/coralscapes/, and coralscapesScripts is in <project_root>/\n",
    "# # project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "# # if project_root not in sys.path:\n",
    "# #     sys.path.insert(0, project_root)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# !ls\n",
    "# %cd coralscapesScripts\n",
    "# TODO: specify paths\n",
    "root = \"data_yellow_0\"\n",
    "root_copy = \"data_yellow_0\"\n",
    "color = \"yellow\"  # these are to name the checkpoints folder\n",
    "cluster = \"0\"\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjEK6iQdaMlD",
    "outputId": "7f0dd353-1478-44ff-8e6c-fbcff60161c4",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:39.890650Z",
     "start_time": "2025-10-14T21:28:39.887228Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dms1Nh1a_TTo",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.225919Z",
     "start_time": "2025-10-14T21:28:39.898976Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from coralscapesScripts.datasets.dataset import Coralscapes\n",
    "from coralscapesScripts.datasets.preprocess import get_preprocessor\n",
    "from coralscapesScripts.datasets.utils import calculate_weights\n",
    "\n",
    "from coralscapesScripts.segmentation.model import Benchmark_Run\n",
    "from coralscapesScripts.segmentation.benchmarking import launch_benchmark\n",
    "\n",
    "from coralscapesScripts.visualization import show_samples\n",
    "\n",
    "from coralscapesScripts.logger import Logger, save_benchmark_run\n",
    "from coralscapesScripts.io import setup_config, get_parser, update_config_with_args\n",
    "import copy\n",
    "\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "#their eval sucks coz its bloated so using simpligied on\n",
    "import torch\n",
    "# from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torch.nn as nn # Import the neural network module\n",
    "import torch.nn.functional as F # Import functional for interpolation\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20223018\\.conda\\envs\\jbg060_4\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\20223018\\.conda\\envs\\jbg060_4\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "C:\\Users\\20223018\\.conda\\envs\\jbg060_4\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8Ois-qOM_TTo",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.347461Z",
     "start_time": "2025-10-14T21:28:51.334196Z"
    }
   },
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GCZBHx8E_TTp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a901eb8c-3b98-4674-f778-683aafb8c128",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.386682Z",
     "start_time": "2025-10-14T21:28:51.353471Z"
    }
   },
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "for i in range(device_count):\n",
    "    print(f\"CUDA Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device 0: NVIDIA RTX A2000 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HgwbOAz_TTp"
   },
   "source": [
    "## Config\n",
    "\n",
    "- Currently the Benchmark_Run takes the cfg and pops out the different parts (meaning the cfg file is **changed** after the object initialization).\n",
    "- If you want to look at the cfg after that, use the cfg_logger copy which will contain the original configuration.\n",
    "- Keep in mind due to this that if you try to \"recreate\" the Benchmark_Run object without rerunning the configuration it will most likely result in an error, so better to start over (or at least just rerun the config part of the script)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# dont really get wtvr the fuck they r tryna so imma just roll w it for now\n",
    "# UPDATE- u just have to rerun the whole thing coz it changes teh cfg. nothing major. ignoe"
   ],
   "metadata": {
    "id": "Ckhqz9CvFZQQ"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1XZVYuyq_TTp",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.405522Z",
     "start_time": "2025-10-14T21:28:51.388381Z"
    }
   },
   "source": "cfg = setup_config(config_path='configs/segformer-mit-b2_lora.yaml', config_base_path='configs/base.yaml')",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOtX8lJT_TTp"
   },
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OirnEfnk_TTp"
   },
   "source": [
    "### Note\n",
    "- As in the command line, here the arguments take presedence over the config file, so here you can overwrite/test out some parameters without directly changing the cfg.\n",
    "- Keep in mind not all parameters of the config can be changed through cmd (you can see which ones in the get_parser and update_config_with_args function)."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "b8uUNdeXqAee",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.416539Z",
     "start_time": "2025-10-14T21:28:51.413084Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CK8SwNWD_TTp",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.425908Z",
     "start_time": "2025-10-14T21:28:51.420953Z"
    }
   },
   "source": [
    "args_input = \"--run-name=fine_tune_notebook --batch-size=1 --batch-size-eval=1 --epochs=50\"\n",
    "args_input = args_input.split(\" \")\n",
    "\n",
    "parser = get_parser()\n",
    "args = parser.parse_args(args_input)\n",
    "\n",
    "cfg = update_config_with_args(cfg, args)\n",
    "cfg_logger = copy.deepcopy(cfg)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l11O4atK_TTq"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSTD-Biq_TTq"
   },
   "source": [
    "### Note\n",
    "Make a new dataset class using the Coralscapes class in coralscapesScripts.datasets.dataset as an example. Importantly, it needs to contain the following as arguments\n",
    "- transform (albumentations)\n",
    "- transform_target\n",
    "- N_classes\n",
    "- id2label  "
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# root=\"/content/drive/MyDrive/Data Challenge 3 Group 3/01_data/coral_bleaching/reef_support/UNAL_BLEACHING_TAYRONA\"\n",
    "# root=\"/content/drive/MyDrive/Data Challenge 3 Group 3/cluster_2\"\n",
    "\n",
    "id2classes = {\n",
    "    # Background classes -> 0 (background)\n",
    "    #ignore the label name from here, i jut saw it from the class.json we can play around w it but meh\n",
    "    0: 0,   # background\n",
    "    1: 0,   # seagrass\n",
    "    2: 0,   # sand OR WTVR\n",
    "    5: 0,   # sand or wtvr\n",
    "    6: 0,   # rubble or wtvr\n",
    "    7: 0,   # algae_covered_substrate\n",
    "    8: 0,   # algae_covered_substrate\n",
    "    9: 0,   # fish\n",
    "    10: 0,  # algae_covered_substrate\n",
    "    11: 0,  # algae_covered_substrate\n",
    "    12: 0,  # algae_covered_substrate\n",
    "    13: 0,  # background\n",
    "    14: 0,  # dark\n",
    "    15: 0,  # algae_covered_substrate\n",
    "\n",
    "    # Bleached coral classes -> 1 (bleached)\n",
    "    3: 1,   # other_coral_bleached\n",
    "    4: 1,   # other_coral_bleached\n",
    "    16: 1,  # massive_meandering_bleached -> bleached\n",
    "    19: 1,  # branching_bleached -> bleached\n",
    "    33: 1,  # meandering_bleached -> bleached\n",
    "\n",
    "    # Non-bleached coral classes -> 2 (non-bleached)\n",
    "    17: 2,  # massive_meandering_alive -> non-bleached\n",
    "    18: 2,  # rubble (coral rubble) -> non-bleached\n",
    "    20: 2,  # branching_dead -> non-bleached (dead but not bleached)\n",
    "    21: 2,  # millepora -> non-bleached\n",
    "    22: 2,  # branching_alive -> non-bleached\n",
    "    23: 2,  # massive_meandering_dead -> non-bleached (dead but not bleached)\n",
    "    24: 2,  # clam -> non-bleached\n",
    "    25: 2,  # acropora_alive -> non-bleached\n",
    "    26: 2,  # sea_cucumber -> non-bleached\n",
    "    27: 2,  # turbinaria -> non-bleached\n",
    "    28: 2,  # table_acropora_alive -> non-bleached\n",
    "    29: 2,  # sponge -> non-bleached\n",
    "    30: 2,  # anemone -> non-bleached\n",
    "    31: 2,  # pocillopora_alive -> non-bleached\n",
    "    32: 2,  # table_acropora_dead -> non-bleached (dead but not bleached)\n",
    "    34: 2,  # stylophora_alive -> non-bleached\n",
    "    35: 2,  # sea_urchin -> non-bleached\n",
    "    36: 2,  # meandering_alive -> non-bleached\n",
    "    37: 2,  # meandering_dead -> non-bleached (dead but not bleached)\n",
    "    38: 2,  # crown_of_thorn -> non-bleached\n",
    "    39: 2,  # dead_clam -> non-bleached\n",
    "}"
   ],
   "metadata": {
    "id": "m91sFkHSMuee",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.435725Z",
     "start_time": "2025-10-14T21:28:51.429971Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper function to map 40-class predictions back to 3 classes\n",
    "def map_40_to_3_classes(predictions):\n",
    "    \"\"\"\n",
    "    Map 40-class predictions back to 3 classes:\n",
    "    - 0: Background (classes 0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)\n",
    "    - 1: Bleached (classes 3, 4, 16, 19, 33)\n",
    "    - 2: Non-bleached (classes 17, 18, 20-32, 34-39)\n",
    "    \"\"\"\n",
    "    # Create a mapping array\n",
    "    mapping = np.zeros(40, dtype=np.uint8)\n",
    "\n",
    "    # Set bleached classes\n",
    "    bleached_classes = [3, 4, 16, 19, 33]\n",
    "    for cls in bleached_classes:\n",
    "        mapping[cls] = 1\n",
    "\n",
    "    # Set non-bleached classes\n",
    "    non_bleached_classes = [17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39]\n",
    "    for cls in non_bleached_classes:\n",
    "        mapping[cls] = 2\n",
    "\n",
    "    # Apply mapping\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "\n",
    "    mapped_predictions = mapping[predictions]\n",
    "    return mapped_predictions"
   ],
   "metadata": {
    "id": "g9Eul-oVyfTv",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.445092Z",
     "start_time": "2025-10-14T21:28:51.439890Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "# for root, dirs, files in os.walk('/content/drive/MyDrive/Data Challenge 3 Group 3/01_data/coral_bleaching/reef_support/UNAL_BLEACHING_TAYRONA'):\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for name in dirs:\n",
    "        print(os.path.join(root, name), 'idoes it exist?')\n"
   ],
   "metadata": {
    "id": "r95vuEmrI_14",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8450b3f5-7cb5-45e4-ce30-71d6a041d303",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.486898Z",
     "start_time": "2025-10-14T21:28:51.450633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_yellow_0\\images idoes it exist?\n",
      "data_yellow_0\\masks_bleached idoes it exist?\n",
      "data_yellow_0\\masks_non_bleached idoes it exist?\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "class CoralBleachingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for coral bleaching segmentation.\n",
    "    Expects data structure:\n",
    "    - images/: RGB images\n",
    "    - masks_bleached/: Masks for bleached corals\n",
    "    - masks_non_bleached/: Masks for non-bleached corals\n",
    "\n",
    "    This class follows the structure of the original Coralscapes class but works with our 3-class setup.\n",
    "    \"\"\"\n",
    "    def __init__(self, root=root, split='train', transform=None, transform_target=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            transform (callable): A function/transform that takes in an image and returns a transformed version.\n",
    "            transform_target (bool): Whether to also transform the segmentation mask.\n",
    "        \"\"\"\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.transform_target = transform_target\n",
    "        self.split = split\n",
    "\n",
    "        self.CoralClass = namedtuple('CoralClass', ['name', 'id', 'train_id', 'category', 'category_id', 'ignore_in_eval', 'color'])\n",
    "\n",
    "        self.classes = []\n",
    "        self.classes.append(self.CoralClass('background', 0, 0, 'background', 0, True, (0, 0, 0)))      # Black - background\n",
    "        self.classes.append(self.CoralClass('bleached', 1, 1, 'bleached', 1, False, (255, 255, 255)))    # White - bleached coral\n",
    "        self.classes.append(self.CoralClass('non_bleached', 2, 2, 'non_bleached', 2, False, (128, 128, 128)))  # Gray - non-bleached coral\n",
    "\n",
    "        # Set number of classes - follow the same logic as the original Coralscapes class\n",
    "        self.N_classes = 3  # We have 3 classes: background, bleached, non-bleached\n",
    "\n",
    "        self.id2label = {0: \"background\", 1: \"bleached\", 2: \"non_bleached\"}\n",
    "        self.label2id = {v: k for k, v in self.id2label.items()}\n",
    "\n",
    "        self.train_id_to_color = np.array([c.color for c in self.classes])\n",
    "\n",
    "        # juliette double check these.\n",
    "        # Define mapping from original 40 classes to our 3 classes\n",
    "        # This mapping is critical for interfacing with the model\n",
    "        self.class_mapping = {  #Mo\n",
    "            # Background classes -> 0\n",
    "            0: 0, 1: 0, 2: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0,\n",
    "            # Bleached coral classes -> 1\n",
    "            3: 1, 4: 1, 16: 1, 19: 1, 33: 1,\n",
    "            # Non-bleached coral classes -> 2\n",
    "            17: 2, 18: 2, 20: 2, 21: 2, 22: 2, 23: 2, 24: 2, 25: 2, 26: 2, 27: 2, 28: 2, 29: 2,\n",
    "            30: 2, 31: 2, 32: 2, 34: 2, 35: 2, 36: 2, 37: 2, 38: 2, 39: 2\n",
    "        }\n",
    "        # self.class_mapping = {  #mine\n",
    "        #     # Background classes -> 0\n",
    "        #     0: 0, 1: 0, 2: 0, 5: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 18: 2,\n",
    "        #     # Bleached coral classes -> 1\n",
    "        #     3: 1, 4: 1, 16: 1, 19: 1, 33: 1,\n",
    "        #     # Non-bleached coral classes -> 2\n",
    "        #     6: 2,\n",
    "        #     17: 2, 20: 2, 21: 2, 22: 2, 23: 2, 24: 2, 25: 2, 26: 2, 27: 2, 28: 2, 29: 2,\n",
    "        #     30: 2, 31: 2, 32: 2, 34: 2, 35: 2, 36: 2, 37: 2, 38: 2, 39: 2\n",
    "        # }\n",
    "        # Create reverse mapping for model output interpretation\n",
    "        self.reverse_class_mapping = {}\n",
    "        for original_class, our_class in self.class_mapping.items():\n",
    "            if our_class not in self.reverse_class_mapping:\n",
    "                self.reverse_class_mapping[our_class] = []\n",
    "            self.reverse_class_mapping[our_class].append(original_class)\n",
    "\n",
    "        # Create train/val/test splits\n",
    "        self.images_dir = os.path.join(self.root, 'images')\n",
    "        all_images = [f for f in os.listdir(self.images_dir)\n",
    "                     if f.endswith(('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG'))]\n",
    "\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(all_images)\n",
    "\n",
    "        if split == 'train':\n",
    "            self.images = all_images[:int(0.7*len(all_images))]\n",
    "        elif split == 'val':\n",
    "            self.images = all_images[int(0.7*len(all_images)):int(0.85*len(all_images))]\n",
    "        elif split == 'test':\n",
    "            self.images = all_images[int(0.85*len(all_images)):]\n",
    "\n",
    "        print(f\"Loaded {len(self.images)} images for {split} split\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieve and transform an image and its corresponding segmentation maps.\n",
    "        \"\"\"\n",
    "        img_name = self.images[index]\n",
    "        image_path = os.path.join(self.root, 'images', img_name)\n",
    "\n",
    "        # Get the base name without extension\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "\n",
    "        # Load image\n",
    "        image = np.array(Image.open(image_path).convert('RGB'))\n",
    "\n",
    "        # Load bleached and non-bleached masks\n",
    "        bleached_mask_path = os.path.join(self.root, 'masks_bleached', f\"{base_name}_bleached.png\")\n",
    "        non_bleached_mask_path = os.path.join(self.root, 'masks_non_bleached', f\"{base_name}_non_bleached.png\")\n",
    "\n",
    "        try:\n",
    "            bleached_mask = np.array(Image.open(bleached_mask_path).convert('L'))\n",
    "            non_bleached_mask = np.array(Image.open(non_bleached_mask_path).convert('L'))\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error loading mask for {img_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Create a combined mask where:\n",
    "        # 0 = background, 1 = bleached, 2 = non-bleached\n",
    "        target = np.zeros_like(bleached_mask, dtype=np.uint8)\n",
    "        target[bleached_mask > 128] = 1      # White pixels in bleached mask -> class 1\n",
    "        target[non_bleached_mask > 128] = 2  # White pixels in non-bleached mask -> class 2\n",
    "        # Background remains 0 (black pixels)\n",
    "\n",
    "        # ADD THIS: Resize both image and mask to the same size\n",
    "        target_size = (512, 512)  # or whatever size you want\n",
    "\n",
    "        # Resize image\n",
    "        image_pil = Image.fromarray(image)\n",
    "        image_resized = image_pil.resize(target_size)\n",
    "        image = np.array(image_resized)\n",
    "\n",
    "        # Resize mask\n",
    "        target_pil = Image.fromarray(target)\n",
    "        target_resized = target_pil.resize(target_size, Image.NEAREST)  # Use NEAREST for masks\n",
    "        target = np.array(target_resized)\n",
    "\n",
    "        if self.transform:\n",
    "            if self.transform_target:\n",
    "                transformed = self.transform(image=image, mask=target)\n",
    "                image = transformed[\"image\"]\n",
    "                target = transformed[\"mask\"]\n",
    "            else:\n",
    "                transformed = self.transform(image=image)\n",
    "                image = transformed[\"image\"]\n",
    "\n",
    "        # Convert image to PyTorch format (C, H, W) if it's still a numpy array\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = image.transpose(2, 0, 1)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ],
   "metadata": {
    "id": "PIjQ-zERFvGB",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.510495Z",
     "start_time": "2025-10-14T21:28:51.492320Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C7xrsEyc_TTq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c48eaf99-ea0b-4b0c-a349-f5df431233f2",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.525523Z",
     "start_time": "2025-10-14T21:28:51.516508Z"
    }
   },
   "source": [
    "#resizing ffs coz images are shite.\n",
    "transforms = {}\n",
    "for split in cfg.augmentation:\n",
    "    transform_list = [A.Resize(height=512, width=512)] # start with resize to ensure consistent size\n",
    "\n",
    "    for transform_name, transform_params in cfg.augmentation[split].items():\n",
    "        if \"Crop\" in transform_name and \"height\" in transform_params and \"width\" in transform_params:# skipping any crop operations that would be larger than our resize\n",
    "            if transform_params[\"height\"] > 512 or transform_params[\"width\"] > 512:\n",
    "                print(f\"Warning: Adjusting '{transform_name}' to match 512x512 size in split '{split}'\")\n",
    "                transform_params[\"height\"] = min(transform_params[\"height\"], 512)\n",
    "                transform_params[\"width\"] = min(transform_params[\"width\"], 512)\n",
    "\n",
    "        transform_list.append(getattr(A, transform_name)(**transform_params))\n",
    "\n",
    "    transforms[split] = A.Compose(transform_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Adjusting 'RandomCrop' to match 512x512 size in split 'train'\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "# cfg.data.root = '/content/drive/MyDrive/Data Challenge 3 Group 3/01_data/coral_bleaching/reef_support/UNAL_BLEACHING_TAYRONA'\n",
    "  # Adjust this path to your data directory FOR EACH OF THE CLUSTER/ this is v important or wasted gpu hours...\n",
    "# cfg.data.root=\"/content/drive/MyDrive/Data Challenge 3 Group 3/cluster_2\"\n",
    "cfg.data.root = root_copy\n"
   ],
   "metadata": {
    "id": "Jnsp08Z4KT0q",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.546467Z",
     "start_time": "2025-10-14T21:28:51.540744Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nbUEn99B_TTq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "524a70f5-c24f-4012-81f7-160f0a133be8",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.561177Z",
     "start_time": "2025-10-14T21:28:51.550844Z"
    }
   },
   "source": [
    "train_dataset = CoralBleachingDataset(root=cfg.data.root, split='train', transform=transforms[\"train\"])\n",
    "\n",
    "transform_target = cfg.training.eval.transform_target if cfg.training.eval is not None and cfg.training.eval.transform_target is not None else True\n",
    "\n",
    "val_dataset = CoralBleachingDataset(root=cfg.data.root, split='val', transform=transforms[\"val\"], transform_target=transform_target)\n",
    "test_dataset = CoralBleachingDataset(root=cfg.data.root, split='test', transform=transforms[\"test\"], transform_target=transform_target)\n",
    "\n",
    "print(f\"Classes: {train_dataset.id2label}\")\n",
    "print(f\"Number of classes: {train_dataset.N_classes}\")\n",
    "print(f\"Train dataset has {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset has {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset has {len(test_dataset)} samples\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 179 images for train split\n",
      "Loaded 38 images for val split\n",
      "Loaded 39 images for test split\n",
      "Classes: {0: 'background', 1: 'bleached', 2: 'non_bleached'}\n",
      "Number of classes: 3\n",
      "Train dataset has 179 samples\n",
      "Validation dataset has 38 samples\n",
      "Test dataset has 39 samples\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zwIFsgsu_TTq",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.576158Z",
     "start_time": "2025-10-14T21:28:51.571279Z"
    }
   },
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=cfg.data.batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.data.batch_size_eval, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.data.batch_size_eval, shuffle=False, num_workers=0)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S80fDtZF_TTq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "606dbf6a-79bc-4932-e6ca-4f7e600e4917",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.594060Z",
     "start_time": "2025-10-14T21:28:51.589243Z"
    }
   },
   "source": [
    "print(len(train_loader), len(val_loader), len(test_loader))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 38 39\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JQJfxQZr_TTq",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.619629Z",
     "start_time": "2025-10-14T21:28:51.614121Z"
    }
   },
   "source": [
    "weight = calculate_weights(train_dataset).to(device) if(cfg.data.weight) else None"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some masks folder sanity checks"
   ],
   "metadata": {
    "id": "Ljq0HuVZRjW0"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yl8QIRQ_TTr"
   },
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i7NbAFeh_TTr",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.629122Z",
     "start_time": "2025-10-14T21:28:51.619629Z"
    }
   },
   "source": [
    "# show_samples(train_dataset, n=2)\n",
    "# wont run coz i changed the class to coralscapebleachingdataset but can be visualized another way, not important yet."
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xdTFpqwB_TTr",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.637462Z",
     "start_time": "2025-10-14T21:28:51.634098Z"
    }
   },
   "source": [
    "# show_samples(val_dataset, n=2)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qxp97wx__TTr",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:51.644656Z",
     "start_time": "2025-10-14T21:28:51.641558Z"
    }
   },
   "source": [
    "# show_samples(test_dataset, n=2)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhwnAI9s_TTr"
   },
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGQdggBG_TTr"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GDTafi9E_TTr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4abbd076-f0e5-45cd-b60d-e3fbb323138e",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:52.971361Z",
     "start_time": "2025-10-14T21:28:51.651224Z"
    }
   },
   "source": [
    "benchmark_run = Benchmark_Run(run_name=cfg.run_name,\n",
    "                             model_name=cfg.model.name,\n",
    "                             N_classes=3,\n",
    "                             device=device,\n",
    "                             model_kwargs=cfg.model.kwargs,\n",
    "                             model_checkpoint=cfg.model.checkpoint,\n",
    "                             lora_kwargs=cfg.lora,\n",
    "                             training_hyperparameters=cfg.training)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20223018\\.conda\\envs\\jbg060_4\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'ignore_index', 'num_labels'\n",
      "  return func(*args, **kwargs)\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b2 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "benchmark_run.print_trainable_parameters()"
   ],
   "metadata": {
    "id": "wt12XyOCOsom",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7f29739b-c63f-4729-c8f9-bb2141bb3f83",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:53.029040Z",
     "start_time": "2025-10-14T21:28:53.022195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5290883 || all params: 32631494 || trainable%: 16.21\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9KDuTbb_TTs"
   },
   "source": [
    "### Note\n",
    "- If you want to use a LoRA approach, as long as you have it in the config (as in this case) with the correct parameters, you are good to go to the training script, no other changes need to be made.  \n",
    "- If you want to freeze a specific layer, the easiest start is to just look at the model here and freeze the parameters you prefer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Dj5N9iTe_TTs",
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "96c60900-4f9a-4f3b-a17e-b973ef690806",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:53.057942Z",
     "start_time": "2025-10-14T21:28:53.044517Z"
    }
   },
   "source": [
    "benchmark_run.model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): SegformerForSemanticSegmentation(\n",
       "      (segformer): SegformerModel(\n",
       "        (encoder): SegformerEncoder(\n",
       "          (patch_embeddings): ModuleList(\n",
       "            (0): SegformerOverlapPatchEmbeddings(\n",
       "              (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "              (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): SegformerOverlapPatchEmbeddings(\n",
       "              (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): SegformerOverlapPatchEmbeddings(\n",
       "              (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): SegformerOverlapPatchEmbeddings(\n",
       "              (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (block): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=64, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=64, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=64, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=64, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "                    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=64, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=64, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=64, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=64, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "                    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.006666666828095913)\n",
       "                (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (2): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=64, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=64, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=64, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=64, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "                    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.013333333656191826)\n",
       "                (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "                    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.019999999552965164)\n",
       "                (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "                    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.02666666731238365)\n",
       "                (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (2): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "                    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.03333333507180214)\n",
       "                (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (3): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=128, out_features=128, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "                    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.03999999910593033)\n",
       "                (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "                    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.046666666865348816)\n",
       "                (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "                    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.0533333346247673)\n",
       "                (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (2): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "                    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.06000000238418579)\n",
       "                (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (3): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "                    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.06666666269302368)\n",
       "                (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (4): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "                    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.07333333790302277)\n",
       "                (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (5): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=320, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "                    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.07999999821186066)\n",
       "                (layer_norm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): ModuleList(\n",
       "              (0): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=512, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=512, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.08666666597127914)\n",
       "                (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=512, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=512, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.09333333373069763)\n",
       "                (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (2): SegformerLayer(\n",
       "                (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SegformerAttention(\n",
       "                  (self): SegformerEfficientSelfAttention(\n",
       "                    (query): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=512, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (value): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=512, out_features=128, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=128, out_features=512, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SegformerSelfOutput(\n",
       "                    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SegformerDropPath(p=0.10000000149011612)\n",
       "                (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): SegformerMixFFN(\n",
       "                  (dense1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (dwconv): SegformerDWConv(\n",
       "                    (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                  (dense2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): ModuleList(\n",
       "            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decode_head): ModulesToSaveWrapper(\n",
       "        (original_module): SegformerDecodeHead(\n",
       "          (linear_c): ModuleList(\n",
       "            (0): SegformerMLP(\n",
       "              (proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "            )\n",
       "            (1): SegformerMLP(\n",
       "              (proj): Linear(in_features=128, out_features=768, bias=True)\n",
       "            )\n",
       "            (2): SegformerMLP(\n",
       "              (proj): Linear(in_features=320, out_features=768, bias=True)\n",
       "            )\n",
       "            (3): SegformerMLP(\n",
       "              (proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (classifier): Conv2d(768, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): SegformerDecodeHead(\n",
       "            (linear_c): ModuleList(\n",
       "              (0): SegformerMLP(\n",
       "                (proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "              )\n",
       "              (1): SegformerMLP(\n",
       "                (proj): Linear(in_features=128, out_features=768, bias=True)\n",
       "              )\n",
       "              (2): SegformerMLP(\n",
       "                (proj): Linear(in_features=320, out_features=768, bias=True)\n",
       "              )\n",
       "              (3): SegformerMLP(\n",
       "                (proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): ReLU()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (classifier): Conv2d(768, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iflqvAxw_TTs"
   },
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install --upgrade wandb\n",
    "# import wandb\n"
   ],
   "metadata": {
    "id": "xwuBVKUHo60w",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:53.085542Z",
     "start_time": "2025-10-14T21:28:53.077375Z"
    }
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wpnknyyI_TTs",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "outputId": "ae3c52e4-5ccc-4a1d-945e-67262b0a8d1b",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:57.688112Z",
     "start_time": "2025-10-14T21:28:53.098353Z"
    }
   },
   "source": [
    "logger = Logger(\n",
    "    project = cfg.logger.wandb_project,\n",
    "    benchmark_run = benchmark_run,\n",
    "    log_epochs = cfg.logger.log_epochs,\n",
    "    config = cfg_logger,\n",
    "    checkpoint_dir = \"./\"\n",
    ")\n",
    "# wandb api key if they ask for it, put this 9770552259c029b0835547eed1ed7173e8c8eccf"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: muhammadibnerafiq (muhammadibnerafiq-tilburg-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "8a9f54a237099dc052ffa075a775b0ab"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\20223018\\PycharmProjects\\DC3\\coralscapesScripts\\wandb\\run-20251014_232854-fxk3tit7</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/muhammadibnerafiq-tilburg-university/coralscapes/runs/fxk3tit7' target=\"_blank\">fine_tune_notebook</a></strong> to <a href='https://wandb.ai/muhammadibnerafiq-tilburg-university/coralscapes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/muhammadibnerafiq-tilburg-university/coralscapes' target=\"_blank\">https://wandb.ai/muhammadibnerafiq-tilburg-university/coralscapes</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/muhammadibnerafiq-tilburg-university/coralscapes/runs/fxk3tit7' target=\"_blank\">https://wandb.ai/muhammadibnerafiq-tilburg-university/coralscapes/runs/fxk3tit7</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL_mk3W4_TTs"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7X7tkPAM_TTs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8ef5d0f7-d168-4f19-bff1-465cd77650e0",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:57.787955Z",
     "start_time": "2025-10-14T21:28:57.778738Z"
    }
   },
   "source": [
    "benchmark_run.print_trainable_parameters()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5290883 || all params: 32631494 || trainable%: 16.21\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e951540",
    "outputId": "342bea9c-57ff-45ef-ff6c-fd4a6503b77d",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:58.145878Z",
     "start_time": "2025-10-14T21:28:57.819604Z"
    }
   },
   "source": [
    "data_batch = next(iter(train_loader))\n",
    "labels_batch = data_batch[1] # Access labels using index 1\n",
    "\n",
    "print(\"Shape of labels batch:\", labels_batch.shape)\n",
    "print(\"Unique values in labels batch:\", torch.unique(labels_batch))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of labels batch: torch.Size([1, 512, 512])\n",
      "Unique values in labels batch: tensor([0, 1, 2], dtype=torch.uint8)\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f0199d45",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:58.158125Z",
     "start_time": "2025-10-14T21:28:58.151908Z"
    }
   },
   "source": [
    "# benchmark_metrics = launch_benchmark(train_loader, val_loader, test_loader, benchmark_run, logger = logger)\n",
    "# save_benchmark_run(benchmark_run, benchmark_metrics)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "WITHOUT K-FOLD COZ K-FOLD IS TOO MUCH..."
   ],
   "metadata": {
    "id": "wkSfCoTmOcL9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_iou(pred, target, num_classes=3):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().float().item()\n",
    "        union = (pred_inds | target_inds).sum().float().item()\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        ious.append(iou)\n",
    "\n",
    "    return ious, np.mean(ious)\n",
    "\n",
    "def train_and_validate_simplified(train_loader, val_loader, benchmark_run, criterion, logger=None, start_epoch=0, end_epoch=None, save_dir=\"./checkpoints\", save_epochs=None):\n",
    "    if end_epoch is None:\n",
    "        end_epoch = benchmark_run.training_hyperparameters.epochs\n",
    "\n",
    "    benchmark_metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": [],\n",
    "        \"val_miou\": [],\n",
    "        \"val_iou_background\": [],\n",
    "        \"val_iou_bleached\": [],\n",
    "        \"val_iou_non_bleached\": []\n",
    "    }\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        print(f\"EPOCH {epoch+1}:\")\n",
    "\n",
    "        # Train\n",
    "        benchmark_run.model.train() # Ensure model is in training mode\n",
    "\n",
    "        train_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for data in tqdm(train_loader, desc=\"Training\"):\n",
    "            # Move data to device\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(benchmark_run.device)\n",
    "            labels = labels.to(benchmark_run.device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            benchmark_run.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = benchmark_run.model(inputs)\n",
    "\n",
    "            # Resize model output (logits) to match target size\n",
    "            # Assuming outputs.logits has shape (N, C, H_out, W_out) and labels have shape (N, H_target, W_target)\n",
    "            # We want to resize outputs.logits to (N, C, H_target, W_target)\n",
    "            resized_logits = F.interpolate(\n",
    "                outputs.logits,\n",
    "                size=labels.shape[-2:], # Target spatial size (H, W)\n",
    "                mode='bilinear', # Use bilinear interpolation for resizing logits\n",
    "                align_corners=False # Avoid alignment issues\n",
    "            )\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(resized_logits, labels.long())\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            benchmark_run.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        if num_batches > 0:\n",
    "            train_loss /= num_batches\n",
    "\n",
    "        benchmark_metrics[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        if hasattr(benchmark_run, \"scheduler\"):\n",
    "            benchmark_run.scheduler.step()\n",
    "\n",
    "        print(f'LOSS train {train_loss}')\n",
    "\n",
    "        # Simple validation pass\n",
    "        benchmark_run.model.eval()\n",
    "        val_loss = 0.0\n",
    "        num_batches_val = 0\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_ious = np.zeros(3)  # For 3 classes\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for vdata in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = vdata\n",
    "                inputs = inputs.to(benchmark_run.device)\n",
    "                labels = labels.to(benchmark_run.device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = benchmark_run.model(inputs)\n",
    "\n",
    "                # Resize model output (logits) to match target size\n",
    "                resized_logits = F.interpolate(\n",
    "                    outputs.logits,\n",
    "                    size=labels.shape[-2:], # Target spatial size (H, W)\n",
    "                    mode='bilinear', # Use bilinear interpolation for resizing logits\n",
    "                    align_corners=False # Avoid alignment issues\n",
    "                )\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(resized_logits, labels.long())\n",
    "\n",
    "                # Calculate accuracy\n",
    "                preds = torch.argmax(resized_logits, dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.numel()\n",
    "\n",
    "                # Calculate IoU\n",
    "                batch_ious, _ = calculate_iou(preds, labels)\n",
    "                val_ious += np.array(batch_ious)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                num_batches_val += 1\n",
    "\n",
    "        if num_batches_val > 0:\n",
    "            val_loss /= num_batches_val\n",
    "            val_ious /= num_batches_val\n",
    "\n",
    "        val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "        val_miou = np.mean(val_ious)\n",
    "\n",
    "\n",
    "        benchmark_metrics[\"val_loss\"].append(val_loss)\n",
    "        benchmark_metrics[\"val_accuracy\"].append(val_accuracy)\n",
    "        benchmark_metrics[\"val_miou\"].append(val_miou)\n",
    "        benchmark_metrics[\"val_iou_background\"].append(val_ious[0])\n",
    "        benchmark_metrics[\"val_iou_bleached\"].append(val_ious[1])\n",
    "        benchmark_metrics[\"val_iou_non_bleached\"].append(val_ious[2])\n",
    "\n",
    "        print(f'LOSS valid {val_loss:.4f}')\n",
    "        print(f'Accuracy valid {val_accuracy:.4f}')\n",
    "        print(f'mIoU valid {val_miou:.4f}')\n",
    "        print(f'IoU per class: Background={val_ious[0]:.4f}, Bleached={val_ious[1]:.4f}, Non-bleached={val_ious[2]:.4f}')\n",
    "\n",
    "        if logger and epoch % logger.log_epochs == 0:\n",
    "            logger.log({\n",
    "                \"train/loss\": train_loss,\n",
    "                \"val/loss\": val_loss,\n",
    "                \"val/accuracy\": val_accuracy,\n",
    "                \"val/miou\": val_miou,\n",
    "                \"val/iou_background\": val_ious[0],\n",
    "                \"val/iou_bleached\": val_ious[1],\n",
    "                \"val/iou_non_bleached\": val_ious[2]\n",
    "            }, epoch)\n",
    "\n",
    "        # Save checkpoint periodically\n",
    "        if save_epochs is not None and (epoch + 1) % save_epochs == 0:\n",
    "            checkpoint_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "            torch.save(benchmark_run.model.state_dict(), checkpoint_path)\n",
    "            print(f\"Saved model checkpoint to {checkpoint_path}\")\n",
    "\n",
    "\n",
    "    metrics_path = os.path.join(save_dir, \"benchmark_metrics.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(benchmark_metrics, f)\n",
    "    return benchmark_metrics"
   ],
   "metadata": {
    "id": "4w1CraC_fI0c",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:58.179612Z",
     "start_time": "2025-10-14T21:28:58.163409Z"
    }
   },
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "# chatgpt version of k-fold should be used ideally\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def calculate_iou(pred, target, num_classes=3):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().float().item()\n",
    "        union = (pred_inds | target_inds).sum().float().item()\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        ious.append(iou)\n",
    "\n",
    "    return ious, np.mean(ious)\n",
    "\n",
    "def train_and_validate_kfold(train_loader, val_loader, benchmark_run, criterion, n_splits=5, logger=None,\n",
    "                            start_epoch=0, end_epoch=None, save_dir=f\"./checkpoints_{color}_{cluster}\", save_best_only=True):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation training and validation.\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        benchmark_run: Object containing model, optimizer, device, etc.\n",
    "        criterion: Loss function\n",
    "        n_splits: Number of folds for k-fold cross-validation\n",
    "        logger: Optional logger for metrics\n",
    "        start_epoch: Starting epoch number\n",
    "        end_epoch: Ending epoch number\n",
    "        save_dir: Directory to save model checkpoints\n",
    "        save_best_only: If True, only save model when validation performance improves\n",
    "    \"\"\"\n",
    "    if end_epoch is None:\n",
    "        end_epoch = benchmark_run.training_hyperparameters.epochs\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Store metrics for each fold\n",
    "    fold_metrics = []\n",
    "\n",
    "    # Get the dataset from the train loader\n",
    "    train_dataset = train_loader.dataset\n",
    "    val_dataset = val_loader.dataset\n",
    "\n",
    "    # Combine datasets for k-fold splitting\n",
    "    # If they're the same dataset with different samplers, just use one\n",
    "    if train_dataset == val_dataset:\n",
    "        full_dataset = train_dataset\n",
    "    else:\n",
    "        # This assumes both datasets have the same structure and can be concatenated\n",
    "        try:\n",
    "            full_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "        except:\n",
    "            print(\"Warning: Could not concatenate datasets. Using training dataset for k-fold.\")\n",
    "            full_dataset = train_dataset\n",
    "\n",
    "    # Initialize k-fold cross validation\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Get batch size and num_workers from original loaders\n",
    "    batch_size = train_loader.batch_size\n",
    "    num_workers = train_loader.num_workers\n",
    "\n",
    "    # Track the best overall model across all folds based on the custom score\n",
    "    best_overall_score = -1.0\n",
    "    best_overall_fold = -1\n",
    "    best_overall_epoch = -1\n",
    "    best_overall_metrics = None\n",
    "    best_overall_model_state_dict = None\n",
    "\n",
    "\n",
    "    # Loop through each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(range(len(full_dataset)))):\n",
    "        print(f\"\\n{'='*20} FOLD {fold+1}/{n_splits} {'='*20}\")\n",
    "\n",
    "        # Create data loaders for this fold\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "        fold_train_loader = torch.utils.data.DataLoader(\n",
    "            full_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=train_subsampler,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        fold_val_loader = torch.utils.data.DataLoader(\n",
    "            full_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=val_subsampler,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        # Reset model weights for each fold if possible\n",
    "        if hasattr(benchmark_run.model, 'reset_parameters'):\n",
    "            benchmark_run.model.reset_parameters()\n",
    "        else:\n",
    "            print(\"Warning: Model does not have reset_parameters method. Using current weights.\")\n",
    "\n",
    "        # Reset optimizer for the new model parameters\n",
    "        if hasattr(benchmark_run, 'optimizer_class') and hasattr(benchmark_run, 'optimizer_params'):\n",
    "            benchmark_run.optimizer = benchmark_run.optimizer_class(\n",
    "                benchmark_run.model.parameters(),\n",
    "                **benchmark_run.optimizer_params\n",
    "            )\n",
    "\n",
    "        # Initialize metrics for this fold\n",
    "        fold_benchmark_metrics = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_accuracy\": [],\n",
    "            \"val_miou\": [],\n",
    "            \"val_iou_background\": [],\n",
    "            \"val_iou_bleached\": [],\n",
    "            \"val_iou_non_bleached\": [],\n",
    "            \"val_custom_score\": []\n",
    "        }\n",
    "\n",
    "        # Track best performance for this fold\n",
    "        best_fold_score = -1.0\n",
    "\n",
    "        # Train for specified number of epochs\n",
    "        for epoch in range(start_epoch, end_epoch):\n",
    "            print(f\"EPOCH {epoch+1}/{end_epoch}:\")\n",
    "\n",
    "            # Train\n",
    "            benchmark_run.model.train()\n",
    "            train_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for data in tqdm(fold_train_loader, desc=\"Training\"):\n",
    "                # Move data to device\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(benchmark_run.device)\n",
    "                labels = labels.to(benchmark_run.device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                benchmark_run.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = benchmark_run.model(inputs)\n",
    "\n",
    "                # Resize model output (logits) to match target size\n",
    "                resized_logits = F.interpolate(\n",
    "                    outputs.logits,\n",
    "                    size=labels.shape[-2:],\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False\n",
    "                )\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(resized_logits, labels.long())\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                benchmark_run.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "            if num_batches > 0:\n",
    "                train_loss /= num_batches\n",
    "\n",
    "            fold_benchmark_metrics[\"train_loss\"].append(train_loss)\n",
    "\n",
    "            if hasattr(benchmark_run, \"scheduler\"):\n",
    "                benchmark_run.scheduler.step()\n",
    "\n",
    "            print(f'LOSS train {train_loss:.4f}')\n",
    "\n",
    "            # Validation\n",
    "            benchmark_run.model.eval()\n",
    "            val_loss = 0.0\n",
    "            num_batches_val = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            val_ious = np.zeros(3)  # For 3 classes\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for vdata in tqdm(fold_val_loader, desc=\"Validating\"):\n",
    "                    inputs, labels = vdata\n",
    "                    inputs = inputs.to(benchmark_run.device)\n",
    "                    labels = labels.to(benchmark_run.device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = benchmark_run.model(inputs)\n",
    "\n",
    "                    # Resize model output (logits) to match target size\n",
    "                    resized_logits = F.interpolate(\n",
    "                        outputs.logits,\n",
    "                        size=labels.shape[-2:],\n",
    "                        mode='bilinear',\n",
    "                        align_corners=False\n",
    "                    )\n",
    "\n",
    "                    # Calculate loss\n",
    "                    loss = criterion(resized_logits, labels.long())\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    preds = torch.argmax(resized_logits, dim=1)\n",
    "                    val_correct += (preds == labels).sum().item()\n",
    "                    val_total += labels.numel()\n",
    "\n",
    "                    # Calculate IoU\n",
    "                    batch_ious, _ = calculate_iou(preds, labels)\n",
    "                    val_ious += np.array(batch_ious)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    num_batches_val += 1\n",
    "\n",
    "            if num_batches_val > 0:\n",
    "                val_loss /= num_batches_val\n",
    "                val_ious /= num_batches_val\n",
    "\n",
    "            val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "            val_miou = np.mean(val_ious)\n",
    "\n",
    "            # Calculate custom ranking score\n",
    "            # rank = alpha * accuracy + beta * mIoU\n",
    "            # mIoU = 0.1 * iou_background + 0.45 * iou_bleached + 0.45 * iou_non_bleached\n",
    "            alpha = 0.2\n",
    "            beta = 0.8\n",
    "            miou_weighted = (0.1 * val_ious[0] + 0.45 * val_ious[1] + 0.45 * val_ious[2])\n",
    "            custom_score = alpha * val_accuracy + beta * miou_weighted\n",
    "\n",
    "            # Store metrics\n",
    "            fold_benchmark_metrics[\"val_loss\"].append(val_loss)\n",
    "            fold_benchmark_metrics[\"val_accuracy\"].append(val_accuracy)\n",
    "            fold_benchmark_metrics[\"val_miou\"].append(val_miou)\n",
    "            fold_benchmark_metrics[\"val_iou_background\"].append(val_ious[0])\n",
    "            fold_benchmark_metrics[\"val_iou_bleached\"].append(val_ious[1])\n",
    "            fold_benchmark_metrics[\"val_iou_non_bleached\"].append(val_ious[2])\n",
    "            fold_benchmark_metrics[\"val_custom_score\"].append(custom_score)\n",
    "\n",
    "\n",
    "            print(f'LOSS valid {val_loss:.4f}')\n",
    "            print(f'Accuracy valid {val_accuracy:.4f}')\n",
    "            print(f'mIoU valid {val_miou:.4f}')\n",
    "            print(f'IoU per class: Background={val_ious[0]:.4f}, Bleached={val_ious[1]:.4f}, Non-bleached={val_ious[2]:.4f}')\n",
    "            print(f'Custom Score valid {custom_score:.4f}')\n",
    "\n",
    "\n",
    "            # Log metrics if logger is provided\n",
    "            if logger and epoch % logger.log_epochs == 0:\n",
    "                logger.log({\n",
    "                    f\"fold_{fold+1}/train/loss\": train_loss,\n",
    "                    f\"fold_{fold+1}/val/loss\": val_loss,\n",
    "                    f\"fold_{fold+1}/val/accuracy\": val_accuracy,\n",
    "                    f\"fold_{fold+1}/val/miou\": val_miou,\n",
    "                    f\"fold_{fold+1}/val/iou_background\": val_ious[0],\n",
    "                    f\"fold_{fold+1}/val/iou_bleached\": val_ious[1],\n",
    "                    f\"fold_{fold+1}/val/iou_non_bleached\": val_ious[2],\n",
    "                    f\"fold_{fold+1}/val/custom_score\": custom_score\n",
    "                }, epoch)\n",
    "\n",
    "            # Save best model for this fold based on custom score\n",
    "            if save_best_only:\n",
    "                if custom_score > best_fold_score:\n",
    "                    best_fold_score = custom_score\n",
    "                    checkpoint_path = os.path.join(save_dir, f\"model_fold_{fold+1}_best_score.pth\")\n",
    "                    torch.save(benchmark_run.model.state_dict(), checkpoint_path)\n",
    "                    print(f\"Saved improved model checkpoint to {checkpoint_path} (Custom Score: {custom_score:.4f})\")\n",
    "\n",
    "            # Check if this is the best overall model\n",
    "            if custom_score > best_overall_score:\n",
    "                best_overall_score = custom_score\n",
    "                best_overall_fold = fold + 1\n",
    "                best_overall_epoch = epoch + 1\n",
    "                best_overall_metrics = {\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_accuracy\": val_accuracy,\n",
    "                    \"val_miou\": val_miou,\n",
    "                    \"val_iou_background\": val_ious[0],\n",
    "                    \"val_iou_bleached\": val_ious[1],\n",
    "                    \"val_iou_non_bleached\": val_ious[2],\n",
    "                    \"val_custom_score\": custom_score\n",
    "                }\n",
    "                best_overall_model_state_dict = copy.deepcopy(benchmark_run.model.state_dict())\n",
    "\n",
    "\n",
    "        # Save metrics for this fold\n",
    "        fold_metrics.append(fold_benchmark_metrics)\n",
    "\n",
    "        # Save fold metrics\n",
    "        metrics_path = os.path.join(save_dir, f\"metrics_fold_{fold+1}.json\")\n",
    "        with open(metrics_path, \"w\") as f:\n",
    "            json.dump(fold_benchmark_metrics, f)\n",
    "\n",
    "    # Calculate and save average metrics across all folds\n",
    "    avg_metrics = {\n",
    "        \"avg_val_miou_final\": np.mean([metrics[\"val_miou\"][-1] for metrics in fold_metrics]),\n",
    "        \"avg_val_accuracy_final\": np.mean([metrics[\"val_accuracy\"][-1] for metrics in fold_metrics]),\n",
    "        \"avg_val_iou_background_final\": np.mean([metrics[\"val_iou_background\"][-1] for metrics in fold_metrics]),\n",
    "        \"avg_val_iou_bleached_final\": np.mean([metrics[\"val_iou_bleached\"][-1] for metrics in fold_metrics]),\n",
    "        \"avg_val_iou_non_bleached_final\": np.mean([metrics[\"val_iou_non_bleached\"][-1] for metrics in fold_metrics]),\n",
    "        \"avg_val_custom_score_final\": np.mean([metrics[\"val_custom_score\"][-1] for metrics in fold_metrics]),\n",
    "        \"best_val_miou_across_folds\": np.max([np.max(metrics[\"val_miou\"]) for metrics in fold_metrics]),\n",
    "        \"best_val_custom_score_across_folds\": np.max([np.max(metrics[\"val_custom_score\"]) for metrics in fold_metrics]),\n",
    "        \"fold_metrics\": fold_metrics,\n",
    "        \"best_overall_model\": {\n",
    "            \"fold\": best_overall_fold,\n",
    "            \"epoch\": best_overall_epoch,\n",
    "            \"metrics\": best_overall_metrics\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save the best overall model checkpoint\n",
    "    if best_overall_model_state_dict is not None:\n",
    "        best_overall_checkpoint_path = os.path.join(save_dir, \"best_overall_model.pth\")\n",
    "        torch.save(best_overall_model_state_dict, best_overall_checkpoint_path)\n",
    "        print(f\"\\nSaved best overall model checkpoint to {best_overall_checkpoint_path} (Custom Score: {best_overall_score:.4f})\")\n",
    "\n",
    "\n",
    "    # Save overall metrics\n",
    "    overall_metrics_path = os.path.join(save_dir, \"kfold_metrics.json\")\n",
    "    with open(overall_metrics_path, \"w\") as f:\n",
    "        json.dump(avg_metrics, f)\n",
    "\n",
    "    print(f\"\\nAverage Final mIoU across folds: {avg_metrics['avg_val_miou_final']:.4f}\")\n",
    "    print(f\"Best mIoU across all folds: {avg_metrics['best_val_miou_across_folds']:.4f}\")\n",
    "    print(f\"Average Final Custom Score across folds: {avg_metrics['avg_val_custom_score_final']:.4f}\")\n",
    "    print(f\"Best Custom Score across all folds: {avg_metrics['best_val_custom_score_across_folds']:.4f}\")\n",
    "    print(f\"\\nBest overall model found in Fold {best_overall_fold}, Epoch {best_overall_epoch} with Custom Score: {best_overall_score:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return avg_metrics"
   ],
   "metadata": {
    "id": "a8vgAprSh1jC",
    "ExecuteTime": {
     "end_time": "2025-10-14T21:28:58.229101Z",
     "start_time": "2025-10-14T21:28:58.186092Z"
    }
   },
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1e8d491",
    "outputId": "87d006a7-bf35-4a61-e108-0cb5def91acf",
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-14T21:28:58.238661Z"
    }
   },
   "source": [
    "%%time\n",
    "criterion = nn.CrossEntropyLoss(weight=weight) # Use the calculated weight\n",
    "save_directory = \"./custom_checkpoints\"\n",
    "save_frequency_epochs = 1 # Set to an integer to save checkpoints\n",
    "\n",
    "# benchmark_metrics = train_and_validate_simplified(\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     benchmark_run,\n",
    "#     criterion=criterion,\n",
    "#     logger=logger,\n",
    "#     save_dir=save_directory,\n",
    "#     save_epochs=save_frequency_epochs # Pass the save frequency\n",
    "# )\n",
    "benchmark_metrics = train_and_validate_kfold(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    benchmark_run,\n",
    "    criterion,\n",
    "    n_splits=5,\n",
    "    save_best_only=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== FOLD 1/5 ====================\n",
      "Warning: Model does not have reset_parameters method. Using current weights.\n",
      "EPOCH 1/50:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 49/173 [00:21<00:50,  2.48it/s]"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
